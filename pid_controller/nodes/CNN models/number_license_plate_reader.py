#! /usr/bin/env python

"""Number License Plate Reader.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ybWQTp75tzTvMCgJggxPWcv_cUisnLRp
"""

import math
import numpy as np
import re
import cv2
from collections import Counter
from matplotlib import pyplot as plt
from PIL import Image
from os import listdir


#Load Data
PATH = "/home/fizzer/ros_ws/src/Competition_Package/pid_controller/nodes/CNN models/Cropped and Labelled License Plates"

files = listdir(PATH)
folder = PATH

i = np.random.random_integers(0,100)
path = PATH + '/' + files[i]
img = cv2.imread(path)
h,w,d = img.shape
sec_w = int(w/4)


#Load the images
imgset = np.array(
    [[np.array(Image.open(PATH + '/' + file))[:, sec_w*i:sec_w*(i+1)], file[i]] for file in files
     for i in range(2,4)])

print("Loaded {:} images from folder:\n{}".format(imgset.shape[0], folder))


#Crop out whitespace
i = 0
while(i < imgset.shape[0]):
  imgset[i,0] = imgset[i,0][:,7:25]
  imgset[i + 1,0] = imgset[i + 1,0][:,0:18]
  i = i + 2


# Generate X and Y datasets
np.random.shuffle(imgset)
X_dataset_orig = np.array([data[0] for data in imgset])
Y_dataset_orig = np.array([data[1] for data in imgset])


#Use numbers 0 to 9 for numbers and 10 to 35 for letters
def value(char):
    return char

#Find the character from the value
def char(val):
    return val

NUMBER_OF_LABELS = 10
CONFIDENCE_THRESHOLD = 0.01


def convert_to_one_hot(Y, C):
    Y = np.reshape(Y,(-1))
    Y1 = np.zeros((Y.shape[0], NUMBER_OF_LABELS))
    for c in range(Y1.shape[0]):
      Y1[c,int(value(Y[c]))] = 1
    return Y1
  
# Normalize X (images) dataset
X_dataset = X_dataset_orig/255.

# Convert Y dataset to one-hot encoding
Y_dataset = convert_to_one_hot(Y_dataset_orig, NUMBER_OF_LABELS)


#Split Dataset and begin the MODELLING
VALIDATION_SPLIT = 0.2

from keras import layers
from keras import models
from keras import optimizers

from keras.utils import plot_model
from keras import backend

from keras import models

#from sklearn.metrics import confusion_matrix, plot_confusion_matrix

# Source: https://stackoverflow.com/questions/63435679
def reset_weights(model):
  for ix, layer in enumerate(model.layers):
      if (hasattr(model.layers[ix], 'kernel_initializer') and 
          hasattr(model.layers[ix], 'bias_initializer')):
          weight_initializer = model.layers[ix].kernel_initializer
          bias_initializer = model.layers[ix].bias_initializer

          old_weights, old_biases = model.layers[ix].get_weights()

          model.layers[ix].set_weights([
              weight_initializer(shape=old_weights.shape),
              bias_initializer(shape=len(old_biases))])

#Create CNN Model
conv_model = models.Sequential()
conv_model.add(layers.Conv2D(10, (3, 3), activation='relu',
                             input_shape=(20, 18, 3)))
conv_model.add(layers.MaxPooling2D((2, 2)))
conv_model.add(layers.Conv2D(20, (3, 3), activation='relu'))
conv_model.add(layers.MaxPooling2D((2, 2)))
conv_model.add(layers.Flatten())
conv_model.add(layers.Dropout(0.5))
conv_model.add(layers.Dense(120, activation='relu'))
conv_model.add(layers.Dense(84, activation='relu'))
conv_model.add(layers.Dense(10, activation='softmax'))

#Model Summary
conv_model.summary()

#Compile and Train
LEARNING_RATE = 7e-4
conv_model.compile(loss='categorical_crossentropy',
                   optimizer=optimizers.RMSprop(lr=LEARNING_RATE),
                   metrics=['acc'])

#reset_weights(conv_model)
history_conv = conv_model.fit(X_dataset, Y_dataset, 
                              validation_split=VALIDATION_SPLIT, 
                              epochs=100, 
                              batch_size=4)

#Plot Losses
plt.plot(history_conv.history['loss'])
plt.plot(history_conv.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train loss', 'val loss'], loc='upper left')
plt.show()

#Plot Accuracy
plt.plot(history_conv.history['acc'])
plt.plot(history_conv.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy (%)')
plt.xlabel('epoch')
plt.legend(['train accuracy', 'val accuracy'], loc='upper left')
plt.show()

'''
#Plot confusion matrix
import pandas as pd
import seaborn as sn

label = '0123456789'
y_pred = conv_model.predict(X_dataset)

def get_char(position):
  if position < 26:
     return chr(position + ord('A'))
  else:
    return chr(position - 26 + ord('0'))

y_pred_chars = [[get_char(np.argmax(pred))] for pred in y_pred]

y_true_chars = [[get_char(np.where(y_true == 1)[0])] for y_true in Y_dataset]


cm = confusion_matrix(y_true_chars, y_pred_chars)

df_cm = pd.DataFrame(cm, index = [i for i in label],
                  columns = [i for i in label])
plt.figure(figsize = (10,7))
sn.heatmap(df_cm, annot=True)
'''

#Saving model
models.save_model(conv_model,'/home/fizzer/ros_ws/src/Competition_Package/pid_controller/nodes/CNN models/NumberModel')